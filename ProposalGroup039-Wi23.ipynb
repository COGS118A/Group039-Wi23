{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer Review\n",
    "\n",
    "You will all have an opportunity to look at the Project Proposals of other groups to fuel your creativity and get more ideas for how you can improve your own projects. \n",
    "\n",
    "Both the project proposal and project checkpoint will have peer review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Abdulrahman Sinjab\n",
    "- Muhammad Fadli Alim Arsani\n",
    "- Sarah Kagzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We plan to experiement with the following dataset to achieve the goal of this project: Detecting Fraudulent Transactions In Cryotocurrencies Exchange. The potential dataset(s) are as follows:\n",
    "- [Ethereum Fraud Detection Dataset](https://www.kaggle.com/datasets/vagifa/ethereum-frauddetection-dataset?datasetId=1074447&sortBy=voteCounthttps://www.kaggle.com/datasets/vagifa/ethereum-frauddetection-dataset?datasetId=1074447&sortBy=voteCount)\n",
    "- [Elliptic Dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set)\n",
    "\n",
    "__Ethereum Fraud Detection Dataset:__\n",
    "\n",
    "This dataset contains rows of known fraud and valid transactions made over Ethereum, a type of cryptocurrency.\n",
    "\n",
    "Each observation consists of the following features:\n",
    "- Index: the index number of a row\n",
    "- Address: the address of the ethereum account\n",
    "- FLAG: whether the transaction is fraud or not\n",
    "- Avg min between sent tnx: Average time between sent transactions for account in minutes\n",
    "- Avg_min_between_received_tnx: Average time between received transactions for account in minutes\n",
    "- Time_Diff_between_first_and_last(Mins): Time difference between the first and last transaction\n",
    "- Sent_tnx: Total number of sent normal transactions\n",
    "- Received_tnx: Total number of received normal transactions\n",
    "- Number_of_Created_Contracts: Total Number of created contract transactions\n",
    "- Unique_Received_From_Addresses: Total Unique addresses from which account received transactions\n",
    "- Unique_Sent_To_Addresses20: Total Unique addresses from which account sent transactions\n",
    "- Min_Value_Received: Minimum value in Ether ever received\n",
    "- Max_Value_Received: Maximum value in Ether ever received\n",
    "- Avg_Value_Received5Average value in Ether ever received\n",
    "- Min_Val_Sent: Minimum value of Ether ever sent\n",
    "- Max_Val_Sent: Maximum value of Ether ever sent\n",
    "- Avg_Val_Sent: Average value of Ether ever sent\n",
    "- Min_Value_Sent_To_Contract: Minimum value of Ether sent to a contract\n",
    "- Max_Value_Sent_To_Contract: Maximum value of Ether sent to a contract\n",
    "- Avg_Value_Sent_To_Contract: Average value of Ether sent to contracts\n",
    "- Total_Transactions(Including_Tnx_to_Create_Contract): Total number of transactions\n",
    "- Total_Ether_Sent:Total Ether sent for account address\n",
    "- Total_Ether_Received: Total Ether received for account address\n",
    "- Total_Ether_Sent_Contracts: Total Ether sent to Contract addresses\n",
    "- Total_Ether_Balance: Total Ether Balance following enacted transactions\n",
    "- Total_ERC20_Tnxs: Total number of ERC20 token transfer transactions\n",
    "- ERC20_Total_Ether_Received: Total ERC20 token received transactions in Ether\n",
    "- ERC20_Total_Ether_Sent: Total ERC20token sent transactions in Ether\n",
    "- ERC20_Total_Ether_Sent_Contract: Total ERC20 token transfer to other contracts in Ether\n",
    "- ERC20_Uniq_Sent_Addr: Number of ERC20 token transactions sent to Unique account addresses\n",
    "- ERC20_Uniq_Rec_Addr: Number of ERC20 token transactions received from Unique addresses\n",
    "- ERC20_Uniq_Rec_Contract_Addr: Number of ERC20token transactions received from Unique contract addresses\n",
    "- ERC20_Avg_Time_Between_Sent_Tnx: Average time between ERC20 token sent transactions in minutes\n",
    "- ERC20_Avg_Time_Between_Rec_Tnx: Average time between ERC20 token received transactions in minutes\n",
    "- ERC20_Avg_Time_Between_Contract_Tnx: Average time ERC20 token between sent token transactions\n",
    "- ERC20_Min_Val_Rec: Minimum value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Max_Val_Rec: Maximum value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Avg_Val_Rec: Average value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Min_Val_Sent: Minimum value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Max_Val_Sent: Maximum value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Avg_Val_Sent: Average value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Uniq_Sent_Token_Name: Number of Unique ERC20 tokens transferred\n",
    "- ERC20_Uniq_Rec_Token_Name: Number of Unique ERC20 tokens received\n",
    "- ERC20_Most_Sent_Token_Type: Most sent token for account via ERC20 transaction\n",
    "- ERC20_Most_Rec_Token_Type: Most received token for account via ERC20 transactions\n",
    "\n",
    "The critical variables in this dataset are yet to be discovered - we will need to perform data inspection on this dataset to infer feature importance.\n",
    "\n",
    "We have inspected the above dataset and found out that the following changes are necessary:\n",
    "- The dataset has severe class imbalance (7662 Non-Fraud, 2179 Fraud), we'll need to resolve this with methods like SMOTE.\n",
    "- Replace missings of numerical variables with median\n",
    "- We will need to drop features with 0 variance since these features will not contribute in the performance of the model.\n",
    "- We also need to drop the group of highly correlated features since they won't bring significant additional information, but instead will increase complexity of the algorithm and demand in compute resource.\n",
    "- Discard features whose values are mostly 0's.\n",
    "- Normalize the features.\n",
    "\n",
    "__Elliptic Dataset:__\n",
    "\n",
    "The Elliptic Dataset is a graph network of Bitcoin transactions with handcrafted features. All features are constructed using only publicly available information.\n",
    "\n",
    "The dataset maps Bitcoin transactions to real entities in two categories:\n",
    "- *Licit:* exchanges, wallet providers, miners, licit services, etc.\n",
    "- *Ilicit:* scams, malware, terrorist, organization, ransomware, Ponzi shcemes, etc\n",
    "\n",
    "A given transaction is licit if the entity that generated it was licit.\n",
    "- *Nodes and Edges:* 203,769 node transactions and 234,355 directed edge payments flows. 2% (4,545) are ilicit (Class 1), 21% (42,019) are licit (Class 2)\n",
    "- *Features:* Each node has associated 166 features. 94 represent local information (timestep, number of inputs/outputs, transaction fee, output volume and aggregated figures) and 72 features represent aggregated features (obtained by aggregating transaction information such as maximum, minimum, standard deviation, correlation coefficients of neighbor transactions).\n",
    "- *Temporal Information:* A time step is associated with each node, representing an stimated of the time when the transaction is confirmed. There are 49 distinct timesteps evenly spaced with an interval of 2 weeks.\n",
    "\n",
    "As far as what are the critical valiables and how they are represented, we do not have a clear opinion on this yet since we have not perform sufficient exploration on the dataset to conclude what features may be of more importance in answering the problem statement. To justify this further, quoting from the dataset provider: *\"Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset\"*. This makes it even harder to read columns from the dataset and infer their importance on answering the problem statement.\n",
    "\n",
    "At the time being, after small exploration on the data, there is only one minor change that we need to do on this dataset: change the column names to something more meaningful.\n",
    "\n",
    "In conclusion, with this dataset, our task is to classify the illicit and licit nodes in the graph (given a node in the graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "We propose a solution using a decision-tree ensemble algorithm, in particular the XGBoost algorithm. We hypothesise that this algorithm will work well on the dataset due to the fact that XGBoost is known to work well on such dataset, i.e: XGBoost is a popular algorithm for binary classification tasks. We can further strengthen this hypothesis by first running multiple ML algorithms on the dataset: logistic regression, decision trees, and random forests. We can determine which one performed best on the dataset via metrics such as precision, recall, and F1 score. Implementing XGBoost on this dataset is as simple as using the XGBoost library.\n",
    "\n",
    "Finally, we will compare this solution with the Light Gradient Boosting Machine (LGBM) approach proposed by Sarthak Et. al. in *\"LGBM: a machine learning approach for Ethereum fraud\n",
    "detection\"* which claims to show slightly better performance (98.60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using a normalized confusion matrix and precision-recall curves to quantify both the benchmark and solution models. We believe these are the most appropriate evaluation metrics because of the fact that we are using classification models as mentioned above (decision trees and logistic regression) based on the context of our data and the fact that we want to have our model forecast and predict, we believe that implementing these two metrics would be most suitable. Additionally, we come to know that these evaluation metrics we chose have the mathematical foundation that makes logical sense for why they would be good evaluation metrics for our problem. For example, the confusion matrix is derived by comparing the predicted labels with the real labels regarding a set of data. The values that are implemented in the confusion matrix are true positive (TP) where the predicted label is positive and the real label is positive, false positive (FP) where the predicted label is positive but the real label is negative, true negative (TN) where the predicted label is negative and the real label is negative, and also false negative (FN) where the predicted label is positive but the real label is positive. For the precision-recall curve it's derived by calculating the precision and recall scores for the model at various threshold levels and it's mathematically represented by plotting percions versus recall at the various thresholds in which the threshold level would determine the probability threshold at which a positive class label is assigned by the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical concerns are very crucial to ensuring successful deployment of our project. These ethical concerns must be taken into account at every stage of this project and these stages are data collection and analysis, data storage, and data modeling. When considering the ethical concerns around the way we collect data for this project we must ensure that there is no data collection bias of our source during the process of collecting data, also, we need to ensure that our data would be appropriately usable for future reproduction and replication. There may be a potential issue in the accuracy of the data due to possible confounds (Unreported values, outliers). We will also analyze the data set for any confidentialities and make sure that the data set excludes the confidential data for specific individuals like for example, the location of where the transactions occurred or received. Now for data storage, the ethical considerations we have to make is that since we will be importing 2 large datasets from kaggle that contain over 10K values. It’s important to verify that each of these values do not reveal the identity of individuals as the data contains information regarding transactions/payments of those individuals. We also need to decide a plan for removing data if we no longer need it. Our plan is to protect our data and to ensure data security is to limit the access to our dataset within our group members only. Since the dataset we use is originally provided from kaggle and not directly from the users, we do not have to worry about including sensitive information because the source, “kaggle” will not ask nor provide their users for it. Also, the above listed sources might have already checked for their own ethical considerations before publishing. For Data Modeling, the ethical consideration we must make is to ensure that the ML model we choose to implement isn’t biased and fits the criteria that we set forth meaning the proposed task we are trying to accomplish. We also want to be careful in minimizing any overfitting so that the model can accurately predict across a large spectrum of diverse inputs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The team expects everyone to meet at least once a week either in person or through google meet (Tuesday, Wednesday, or Friday).\n",
    "- The team expects everyone to communicate through either Discord or text group chat daily.\n",
    "- The team expects everyone to contribute equally, by deciding collectively on the tasks to be completed by when and by who.\n",
    "- The team expects everyone to have access to shared Google Docs, Notebooks and GitHub files.\n",
    "- The team expects everyone to respond to communications within 24 hours.\n",
    "- The team expects everyone to respect each other’s ideas about the project, and accept feedback/criticism.  \n",
    "- If any conflicts arise we will hold a group meeting in which we will make sure to sort out all issues.\n",
    "- We will be making a calendar that corresponds to the deadlines for this project, this will allow us to map out the progress of the project. \n",
    "- If any difficulties arise within our independent tasks we will first try to resolve it as a team and if we can’t then we will reach out for help on piazza. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1:00 PM |  Brainstorm topics/questions, find a couple datasets for each topic of intrest (all)  | Determine best form of communication; Discuss and decide on final project topic; Assign the tasks for the proposal | \n",
    "| 1/24  |  3 pm |  Begin data cleaning (Fadli) | Do the data analysis (Sinjab); Start drafting the project checkpoint (Sarah) | \n",
    "| 1/27  |  1 pm | Just keep working on tasks from before. | Meet together and discuss any issues and start assigning future tasks. | \n",
    "| 2/3  | 3 PM  | Start building the models (Fadli/Sarah/Sinjab) | Discuss progress and any suggestions regarding model implemntation |\n",
    "2/10 - 3/8 |  1 PM  | Work on the actual implementation of the code based on assigned tasks (Fadli/Sarah/Sinjab) | Meet on Discord and discuss progress and any suggestions/feedback regarding code/devolpment process.|\n",
    "| 3/10  | 3 PM  | Complete analysis; Draft results/conclusion/discussion (Fadlie, Sarah, Sinjab) | Give feedback on the work that was comlpleted and begin implementing final draft tasks. |\n",
    "| 3/19  | Before 11:59 PM  | Meet for short brief call on discord to finalize the project before submission. | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
