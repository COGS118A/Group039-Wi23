{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer Review\n",
    "\n",
    "You will all have an opportunity to look at the Project Proposals of other groups to fuel your creativity and get more ideas for how you can improve your own projects. \n",
    "\n",
    "Both the project proposal and project checkpoint will have peer review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Abdulrahman Sinjab\n",
    "- Muhammad Fadli Alim Arsani\n",
    "- Sarah Kagzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "In this project we address the problem of ethereum fraud and classify and predict fraudulent transaction anomalies found in the ethereum network. We do this using two datasets, the ethereum fraud detection dataset and the elliptic dataset, the former containing labelled data on known fraudulent transactions and the latter containing a graph network of bitcoin transactions with a heavy reliance on feature engineering, giving us the opportunity to classify the illicit and licit nodes in the graph. These datasets will first be cleaned, all features will be normalized and features with 0 variance will be dropped. These two datasets will then be used to build a supervised machine learning decision tree ensemble that incorporates XGBoost, logistic regression and random forests in order to effectively classify fraudulent and safe transactions of ethereum. The performance of our model will be measured using a normalized confusion matrix and precision recall curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "We hope to build upon the approach that major banks utilize in their solutions to detect and deal with credit card fraud. For instance, the Visa’s Visa Advanced Authorization system utilizes several features to help it classify a fraudulent transaction<a name=\"tim\"></a>[<sup>[1]</sup>](#timnote). These features include the type of transaction (i.e. whether it was  via mobile, online or contactless), the geolocation, spending patterns (i.e. whether the is transaction too large compared to the cardholder’s usual spending amounts), time of the day (i.e. whether the transaction is too large and occurring at an odd time of the day), etc. Thus we see that an ethereum fraud detection model would also utilize an approach that builds on top of this algorithmic structure of a supervised machine learning classifier. \n",
    "\n",
    "According to a report by the US Federal Trade Commission (FTC), 46,000 people have reported losing over $1 billion in crypto to scams in 2021 <a name=\"staff\"></a>[<sup>[2]</sup>](#staffnote). These fraudulent transactions can be further classified based on type of fraud conducted - for example, investment scams (i.e. fraudulent websites that promise the purchase of ethereum but are actually traps for folks that may not be as knowledgeable about cryptocurrency mining), romance scams (i.e. suspicious websites with tutorials to navigate cryptocurrency that guide users to simply send payments to the scammer), and business and government imposters (i.e. scammers that appear to be trustworthy sources such as the government or a large company). Thus these types of fraud can be used as labels to classify fraudulent and non fraudulent transactions.  \n",
    "\n",
    "An example of similar work that has been done includes a paper by Tehreem Ashfaq et al on a fraud detection mechanism for the bitcoin network in which they use a model incorporating the random forest algorithm and the XGboost algorithm for classification <a name=\"khalid\"></a>[<sup>[3]</sup>](#khalidnote). This model is used to not only classify transactions but also predict future patterns. We hope to incorporate some of their findings and build on it in our own machine learning model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our goal in this project is to detect fraudulent transactions of ethereum. Ethereum and cryptocurrency in general being a disruptive and innovative technology are volatile financial securities, with many looking to invest when the returns are on a high, and many pulling out when their value crashes. Thus, the market for ethereum is a highly volatile one, periodically attracting a high volume of investments. However, since the market for ethereum is still an emerging one and is dependent on disruptive blockchain technology this means that it lacks a centralized regulatory system to certify transactions, thus making it extremely prone to fraud, especially during times when the value of ethereum is at a high. Moreover, since ethereum transactions are irreversible, the consequences of fraud are significantly far reaching and permanent. \n",
    "\n",
    "However, despite the high risk of fraud, cryptocurrency is here to stay and as more people learn about it, is on its way to becoming mainstream as a payment method which makes it a crucial priority for us to find a way to detect fraudulent transactions in order to make ethereum and blockchain technology more secure. We hope to focus on ethereum for this project, however, the machine learning model developed in this project can be generalized for other cryptocurrency too, making our project replicable. Our approach is to use a supervised machine learning model since the detection of fraudulent transactions boils down to classifying fraudulent transactions based on anomalies such as location changes, sudden increases in transactional amounts, and other such user behaviours that are recorded in real time. Machine learning algorithms used for classification would be most effective at detecting any such departures from regular use behaviour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We plan to experiement with the following dataset to achieve the goal of this project: Detecting Fraudulent Transactions In Cryotocurrencies Exchange. The potential dataset(s) are as follows:\n",
    "- [Ethereum Fraud Detection Dataset](https://www.kaggle.com/datasets/vagifa/ethereum-frauddetection-dataset?datasetId=1074447&sortBy=voteCounthttps://www.kaggle.com/datasets/vagifa/ethereum-frauddetection-dataset?datasetId=1074447&sortBy=voteCount)\n",
    "- [Elliptic Dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set)\n",
    "\n",
    "__Ethereum Fraud Detection Dataset:__\n",
    "\n",
    "This dataset contains rows of known fraud and valid transactions made over Ethereum, a type of cryptocurrency.\n",
    "\n",
    "Each observation consists of the following features:\n",
    "- Index: the index number of a row\n",
    "- Address: the address of the ethereum account\n",
    "- FLAG: whether the transaction is fraud or not\n",
    "- Avg min between sent tnx: Average time between sent transactions for account in minutes\n",
    "- Avg_min_between_received_tnx: Average time between received transactions for account in minutes\n",
    "- Time_Diff_between_first_and_last(Mins): Time difference between the first and last transaction\n",
    "- Sent_tnx: Total number of sent normal transactions\n",
    "- Received_tnx: Total number of received normal transactions\n",
    "- Number_of_Created_Contracts: Total Number of created contract transactions\n",
    "- Unique_Received_From_Addresses: Total Unique addresses from which account received transactions\n",
    "- Unique_Sent_To_Addresses20: Total Unique addresses from which account sent transactions\n",
    "- Min_Value_Received: Minimum value in Ether ever received\n",
    "- Max_Value_Received: Maximum value in Ether ever received\n",
    "- Avg_Value_Received5Average value in Ether ever received\n",
    "- Min_Val_Sent: Minimum value of Ether ever sent\n",
    "- Max_Val_Sent: Maximum value of Ether ever sent\n",
    "- Avg_Val_Sent: Average value of Ether ever sent\n",
    "- Min_Value_Sent_To_Contract: Minimum value of Ether sent to a contract\n",
    "- Max_Value_Sent_To_Contract: Maximum value of Ether sent to a contract\n",
    "- Avg_Value_Sent_To_Contract: Average value of Ether sent to contracts\n",
    "- Total_Transactions(Including_Tnx_to_Create_Contract): Total number of transactions\n",
    "- Total_Ether_Sent:Total Ether sent for account address\n",
    "- Total_Ether_Received: Total Ether received for account address\n",
    "- Total_Ether_Sent_Contracts: Total Ether sent to Contract addresses\n",
    "- Total_Ether_Balance: Total Ether Balance following enacted transactions\n",
    "- Total_ERC20_Tnxs: Total number of ERC20 token transfer transactions\n",
    "- ERC20_Total_Ether_Received: Total ERC20 token received transactions in Ether\n",
    "- ERC20_Total_Ether_Sent: Total ERC20token sent transactions in Ether\n",
    "- ERC20_Total_Ether_Sent_Contract: Total ERC20 token transfer to other contracts in Ether\n",
    "- ERC20_Uniq_Sent_Addr: Number of ERC20 token transactions sent to Unique account addresses\n",
    "- ERC20_Uniq_Rec_Addr: Number of ERC20 token transactions received from Unique addresses\n",
    "- ERC20_Uniq_Rec_Contract_Addr: Number of ERC20token transactions received from Unique contract addresses\n",
    "- ERC20_Avg_Time_Between_Sent_Tnx: Average time between ERC20 token sent transactions in minutes\n",
    "- ERC20_Avg_Time_Between_Rec_Tnx: Average time between ERC20 token received transactions in minutes\n",
    "- ERC20_Avg_Time_Between_Contract_Tnx: Average time ERC20 token between sent token transactions\n",
    "- ERC20_Min_Val_Rec: Minimum value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Max_Val_Rec: Maximum value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Avg_Val_Rec: Average value in Ether received from ERC20 token transactions for account\n",
    "- ERC20_Min_Val_Sent: Minimum value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Max_Val_Sent: Maximum value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Avg_Val_Sent: Average value in Ether sent from ERC20 token transactions for account\n",
    "- ERC20_Uniq_Sent_Token_Name: Number of Unique ERC20 tokens transferred\n",
    "- ERC20_Uniq_Rec_Token_Name: Number of Unique ERC20 tokens received\n",
    "- ERC20_Most_Sent_Token_Type: Most sent token for account via ERC20 transaction\n",
    "- ERC20_Most_Rec_Token_Type: Most received token for account via ERC20 transactions\n",
    "\n",
    "The critical variables in this dataset are yet to be discovered - we will need to perform data inspection on this dataset to infer feature importance.\n",
    "\n",
    "We have inspected the above dataset and found out that the following changes are necessary:\n",
    "- The dataset has severe class imbalance (7662 Non-Fraud, 2179 Fraud), we'll need to resolve this with methods like SMOTE.\n",
    "- Replace missings of numerical variables with median\n",
    "- We will need to drop features with 0 variance since these features will not contribute in the performance of the model.\n",
    "- We also need to drop the group of highly correlated features since they won't bring significant additional information, but instead will increase complexity of the algorithm and demand in compute resource.\n",
    "- Discard features whose values are mostly 0's.\n",
    "- Normalize the features.\n",
    "\n",
    "__Elliptic Dataset:__\n",
    "\n",
    "The Elliptic Dataset is a graph network of Bitcoin transactions with handcrafted features. All features are constructed using only publicly available information.\n",
    "\n",
    "The dataset maps Bitcoin transactions to real entities in two categories:\n",
    "- *Licit:* exchanges, wallet providers, miners, licit services, etc.\n",
    "- *Ilicit:* scams, malware, terrorist, organization, ransomware, Ponzi shcemes, etc\n",
    "\n",
    "A given transaction is licit if the entity that generated it was licit.\n",
    "- *Nodes and Edges:* 203,769 node transactions and 234,355 directed edge payments flows. 2% (4,545) are ilicit (Class 1), 21% (42,019) are licit (Class 2)\n",
    "- *Features:* Each node has associated 166 features. 94 represent local information (timestep, number of inputs/outputs, transaction fee, output volume and aggregated figures) and 72 features represent aggregated features (obtained by aggregating transaction information such as maximum, minimum, standard deviation, correlation coefficients of neighbor transactions).\n",
    "- *Temporal Information:* A time step is associated with each node, representing an stimated of the time when the transaction is confirmed. There are 49 distinct timesteps evenly spaced with an interval of 2 weeks.\n",
    "\n",
    "As far as what are the critical valiables and how they are represented, we do not have a clear opinion on this yet since we have not perform sufficient exploration on the dataset to conclude what features may be of more importance in answering the problem statement. To justify this further, quoting from the dataset provider: *\"Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset\"*. This makes it even harder to read columns from the dataset and infer their importance on answering the problem statement.\n",
    "\n",
    "At the time being, after small exploration on the data, there is only one minor change that we need to do on this dataset: change the column names to something more meaningful.\n",
    "\n",
    "In conclusion, with this dataset, our task is to classify the illicit and licit nodes in the graph (given a node in the graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "We propose a solution using a decision-tree ensemble algorithm, in particular the XGBoost algorithm. We hypothesise that this algorithm will work well on the dataset due to the fact that XGBoost is known to work well on such dataset, i.e: XGBoost is a popular algorithm for binary classification tasks. We can further strengthen this hypothesis by first running multiple ML algorithms on the dataset: logistic regression, decision trees, and random forests. We can determine which one performed best on the dataset via metrics such as precision, recall, and F1 score. Implementing XGBoost on this dataset is as simple as using the XGBoost library.\n",
    "\n",
    "Finally, we will compare this solution with the Light Gradient Boosting Machine (LGBM) approach proposed by Sarthak Et. al. in *\"LGBM: a machine learning approach for Ethereum fraud\n",
    "detection\"* which claims to show slightly better performance (98.60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using a normalized confusion matrix and precision-recall curves to quantify both the benchmark and solution models. We believe these are the most appropriate evaluation metrics because of the fact that we are using classification models as mentioned above (decision trees and logistic regression) based on the context of our data and the fact that we want to have our model forecast and predict, we believe that implementing these two metrics would be most suitable. Additionally, we come to know that these evaluation metrics we chose have the mathematical foundation that makes logical sense for why they would be good evaluation metrics for our problem. For example, the confusion matrix is derived by comparing the predicted labels with the real labels regarding a set of data. The values that are implemented in the confusion matrix are true positive (TP) where the predicted label is positive and the real label is positive, false positive (FP) where the predicted label is positive but the real label is negative, true negative (TN) where the predicted label is negative and the real label is negative, and also false negative (FN) where the predicted label is positive but the real label is positive. For the precision-recall curve it's derived by calculating the precision and recall scores for the model at various threshold levels and it's mathematically represented by plotting percions versus recall at the various thresholds in which the threshold level would determine the probability threshold at which a positive class label is assigned by the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical concerns are very crucial to ensuring successful deployment of our project. These ethical concerns must be taken into account at every stage of this project and these stages are data collection and analysis, data storage, and data modeling. When considering the ethical concerns around the way we collect data for this project we must ensure that there is no data collection bias of our source during the process of collecting data, also, we need to ensure that our data would be appropriately usable for future reproduction and replication. There may be a potential issue in the accuracy of the data due to possible confounds (Unreported values, outliers). We will also analyze the data set for any confidentialities and make sure that the data set excludes the confidential data for specific individuals like for example, the location of where the transactions occurred or received. Now for data storage, the ethical considerations we have to make is that since we will be importing 2 large datasets from kaggle that contain over 10K values. It’s important to verify that each of these values do not reveal the identity of individuals as the data contains information regarding transactions/payments of those individuals. We also need to decide a plan for removing data if we no longer need it. Our plan is to protect our data and to ensure data security is to limit the access to our dataset within our group members only. Since the dataset we use is originally provided from kaggle and not directly from the users, we do not have to worry about including sensitive information because the source, “kaggle” will not ask nor provide their users for it. Also, the above listed sources might have already checked for their own ethical considerations before publishing. For Data Modeling, the ethical consideration we must make is to ensure that the ML model we choose to implement isn’t biased and fits the criteria that we set forth meaning the proposed task we are trying to accomplish. We also want to be careful in minimizing any overfitting so that the model can accurately predict across a large spectrum of diverse inputs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The team expects everyone to meet at least once a week either in person or through google meet (Tuesday, Wednesday, or Friday).\n",
    "- The team expects everyone to communicate through either Discord or text group chat daily.\n",
    "- The team expects everyone to contribute equally, by deciding collectively on the tasks to be completed by when and by who.\n",
    "- The team expects everyone to have access to shared Google Docs, Notebooks and GitHub files.\n",
    "- The team expects everyone to respond to communications within 24 hours.\n",
    "- The team expects everyone to respect each other’s ideas about the project, and accept feedback/criticism.  \n",
    "- If any conflicts arise we will hold a group meeting in which we will make sure to sort out all issues.\n",
    "- We will be making a calendar that corresponds to the deadlines for this project, this will allow us to map out the progress of the project. \n",
    "- If any difficulties arise within our independent tasks we will first try to resolve it as a team and if we can’t then we will reach out for help on piazza. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1:00 PM |  Brainstorm topics/questions, find a couple datasets for each topic of intrest (all)  | Determine best form of communication; Discuss and decide on final project topic; Assign the tasks for the proposal | \n",
    "| 1/24  |  3 pm |  Begin data cleaning (Fadli) | Do the data analysis (Sinjab); Start drafting the project checkpoint (Sarah) | \n",
    "| 1/27  |  1 pm | Just keep working on tasks from before. | Meet together and discuss any issues and start assigning future tasks. | \n",
    "| 2/3  | 3 PM  | Start building the models (Fadli/Sarah/Sinjab) | Discuss progress and any suggestions regarding model implemntation |\n",
    "2/10 through 3/8 |  1 PM  | Work on the actual implementation of the code based on assigned tasks (Fadli/Sarah/Sinjab) | Meet on Discord and discuss progress and any suggestions/feedback regarding code/devolpment process.|\n",
    "| 3/10  | 3 PM  | Complete analysis; Draft results/conclusion/discussion (Fadlie, Sarah, Sinjab) | Give feedback on the work that was comlpleted and begin implementing final draft tasks. |\n",
    "| 3/19  | Before 11:59 PM  | Meet for short brief call on discord to finalize the project before submission. | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"timnote\"></a>1.[^](#tim): Maxwell, Tim. “How Major Credit Card Networks Protect Customers against Fraud.” Bankrate, https://www.bankrate.com/finance/credit-cards/major-credit-card-networks-protect-against-fraud/.<br> \n",
    "<a name=\"staffnote\"></a>2.[^](#staff): Staff, the Premerger Notification Office, et al. “Reports Show Scammers Cashing in on Crypto Craze.” Federal Trade Commission, 11 Aug. 2022, https://www.ftc.gov/news-events/data-visualizations/data-spotlight/2022/06/reports-show-scammers-cashing-crypto-craze. \n",
    "\n",
    "<a name=\"khalidnote\"></a>3.[^](#khalid):Khalid, Rabiya, et al. “A Machine Learning and Blockchain Based Efficient Fraud Detection Mechanism.” MDPI, Multidisciplinary Digital Publishing Institute, 21 Sept. 2022, https://www.mdpi.com/1424-8220/22/19/7162.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
